{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NonLinearICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.u_star = data.attr[torch.randperm(data.attr.shape[0])]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, (attr, identity)  = self.data[idx]\n",
    "        image = torch.tensor(np.array(image), dtype=torch.float32).moveaxis(-1, 0)\n",
    "        u = attr\n",
    "        u_star = self.u_star[idx]\n",
    "        return image, u, u_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = torchvision.datasets.CelebA(\"data/CelebA\", split=\"train\", target_type=[\"attr\", \"identity\"])\n",
    "valid = torchvision.datasets.CelebA(\"data/CelebA\", split=\"valid\", target_type=[\"attr\", \"identity\"])\n",
    "test = torchvision.datasets.CelebA(\"data/CelebA\", split=\"test\", target_type=[\"attr\", \"identity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(CelebA_dataset(train), batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(CelebA_dataset(valid), batch_size=batch_size)\n",
    "test_dataloader = DataLoader(CelebA_dataset(test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dimension = 25\n",
    "model = NonLinearICA(3, hidden_dimension, dropout=0.2, data_type='CelebA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=35\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "log_dir = \"logs/CelebA/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "device=\"cuda:9\"\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef10cee75620488ca9e68fb378f483e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training on epoch:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e2e077db8700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train/loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train/accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs), desc=\"Training on epoch\"):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        x, u, u_star = batch\n",
    "        labels = torch.randint(0, 2, size=(x.shape[0], 1), dtype=x.dtype) # choose random u or u_star -> labels\n",
    "        u = torch.where(labels.bool(), u, u_star) # get u or u_star depending on label\n",
    "        \n",
    "        x = x.to(device)\n",
    "        u = u.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        output = model(x, u)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = (torch.sigmoid(output) > 0.5).float()\n",
    "        correct = (pred == labels).float().sum()\n",
    "        writer.add_scalar(\"Train/loss\", loss.cpu().item(), len(train_dataloader)*epoch + i)\n",
    "        writer.add_scalar(\"Train/accuracy\", correct/output.shape[0], len(train_dataloader)*epoch + i)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            x, u, u_star = batch\n",
    "            labels = torch.randint(0, 2, size=(x.shape[0], 1), dtype=x.dtype) # choose random u or u_star -> labels\n",
    "            u = torch.where(labels.bool(), u, u_star) # get u or u_star depending on label\n",
    "\n",
    "            x = x.to(device)\n",
    "            u = u.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(x, u)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.cpu().item()\n",
    "            \n",
    "            pred = (torch.sigmoid(output) > 0.5).float()\n",
    "            val_correct += (pred == labels).float().sum()\n",
    "    writer.add_scalar(\"Test/loss\", val_loss / len(test_dataloader), epoch)\n",
    "    writer.add_scalar(\"Test/accuracy\", val_correct / (len(test_dataloader)*batch_size), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model, log_dir + f\"/model_n{hidden_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonLinearICA(\n",
       "  (h): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): Flatten(start_dim=1, end_dim=-1)\n",
       "    (11): Linear(in_features=79200, out_features=256, bias=True)\n",
       "    (12): ReLU()\n",
       "    (13): Dropout(p=0.2, inplace=False)\n",
       "    (14): Linear(in_features=256, out_features=25, bias=True)\n",
       "  )\n",
       "  (psi): ModuleList(\n",
       "    (0): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (8): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (9): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (10): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (11): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (12): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (13): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (14): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (15): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (16): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (17): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (18): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (19): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (20): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (21): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (22): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (23): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (24): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=41, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = torch.load(log_dir + f\"/model_n{hidden_dimension}\")\n",
    "model_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eac04f5aca44befa64566c8703e5b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:0.351\n",
      "Validation accuracy: 0.866\n"
     ]
    }
   ],
   "source": [
    "model_2.eval()\n",
    "val_loss = 0.0\n",
    "val_correct = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        x, u, u_star = batch\n",
    "        labels = torch.randint(0, 2, size=(x.shape[0], 1), dtype=x.dtype) # choose random u or u_star -> labels\n",
    "        u = torch.where(labels.bool(), u, u_star) # get u or u_star depending on label\n",
    "\n",
    "        x = x.to(device)\n",
    "        u = u.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(x, u)\n",
    "        loss = criterion(output, labels)\n",
    "        val_loss += loss.cpu().item()\n",
    "\n",
    "        pred = (torch.sigmoid(output) > 0.5).float()\n",
    "        val_correct += (pred == labels).float().sum()\n",
    "        \n",
    "print(f\"Validation loss:{val_loss / len(test_dataloader):.3f}\")\n",
    "print(f\"Validation accuracy: {val_correct / (len(test_dataloader)*batch_size):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.u_star = data.targets[torch.randperm(len(data.targets))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, u  = self.data[idx]\n",
    "        u_star = self.u_star[idx]\n",
    "        image = torch.tensor(np.array(image), dtype=torch.float32).unsqueeze(0)\n",
    "        u = nn.functional.one_hot(torch.tensor(u), len(self.data.classes))\n",
    "        u_star = nn.functional.one_hot(u_star, len(self.data.classes))\n",
    "        return image, u, u_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST(\"data/MNIST\", download=True, train=True)\n",
    "test = torchvision.datasets.MNIST(\"data/MNIST\", download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_dataloader = DataLoader(MNIST_dataset(train), batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(MNIST_dataset(test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dimension = 12\n",
    "model = NonLinearICA(1, hidden_dimension, dropout=0.2, data_type='MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=25\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "log_dir = \"logs/MNIST/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "device=\"cuda:9\"\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae91009ab4504c819483a75ce3e134ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training on epoch:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs), desc=\"Training on epoch\"):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        x, u, u_star = batch\n",
    "        labels = torch.randint(0, 2, size=(x.shape[0], 1), dtype=x.dtype) # choose random u or u_star -> labels\n",
    "        u = torch.where(labels.bool(), u, u_star) # get u or u_star depending on label\n",
    "        \n",
    "        x = x.to(device)\n",
    "        u = u.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        output = model(x, u)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = (torch.sigmoid(output) > 0.5).float()\n",
    "        correct = (pred == labels).float().sum()\n",
    "        writer.add_scalar(\"Train/loss\", loss.cpu().item(), len(train_dataloader)*epoch + i)\n",
    "        writer.add_scalar(\"Train/accuracy\", correct/output.shape[0], len(train_dataloader)*epoch + i)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            x, u, u_star = batch\n",
    "            labels = torch.randint(0, 2, size=(x.shape[0], 1), dtype=x.dtype) # choose random u or u_star -> labels\n",
    "            u = torch.where(labels.bool(), u, u_star) # get u or u_star depending on label\n",
    "\n",
    "            x = x.to(device)\n",
    "            u = u.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(x, u)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.cpu().item()\n",
    "            \n",
    "            pred = (torch.sigmoid(output) > 0.5).float()\n",
    "            val_correct += (pred == labels).float().sum()\n",
    "    writer.add_scalar(\"Test/loss\", val_loss / len(test_dataloader), epoch)\n",
    "    writer.add_scalar(\"Test/accuracy\", val_correct / (len(test_dataloader)*batch_size), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model, log_dir + f\"/model_n{hidden_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5_o_Clock_Shadow',\n",
       " 'Arched_Eyebrows',\n",
       " 'Attractive',\n",
       " 'Bags_Under_Eyes',\n",
       " 'Bald',\n",
       " 'Bangs',\n",
       " 'Big_Lips',\n",
       " 'Big_Nose',\n",
       " 'Black_Hair',\n",
       " 'Blond_Hair',\n",
       " 'Blurry',\n",
       " 'Brown_Hair',\n",
       " 'Bushy_Eyebrows',\n",
       " 'Chubby',\n",
       " 'Double_Chin',\n",
       " 'Eyeglasses',\n",
       " 'Goatee',\n",
       " 'Gray_Hair',\n",
       " 'Heavy_Makeup',\n",
       " 'High_Cheekbones',\n",
       " 'Male',\n",
       " 'Mouth_Slightly_Open',\n",
       " 'Mustache',\n",
       " 'Narrow_Eyes',\n",
       " 'No_Beard',\n",
       " 'Oval_Face',\n",
       " 'Pale_Skin',\n",
       " 'Pointy_Nose',\n",
       " 'Receding_Hairline',\n",
       " 'Rosy_Cheeks',\n",
       " 'Sideburns',\n",
       " 'Smiling',\n",
       " 'Straight_Hair',\n",
       " 'Wavy_Hair',\n",
       " 'Wearing_Earrings',\n",
       " 'Wearing_Hat',\n",
       " 'Wearing_Lipstick',\n",
       " 'Wearing_Necklace',\n",
       " 'Wearing_Necktie',\n",
       " 'Young']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.array(train.attr_names) == 'Mustache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/MNIST/\"\n",
    "hidden_dim = 12\n",
    "model = torch.load(\"logs/MNIST/\" + f\"model_n{hidden_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonLinearICA(\n",
       "  (h): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): Flatten(start_dim=1, end_dim=-1)\n",
       "    (11): Linear(in_features=1568, out_features=256, bias=True)\n",
       "    (12): ReLU()\n",
       "    (13): Dropout(p=0.2, inplace=False)\n",
       "    (14): Linear(in_features=256, out_features=12, bias=True)\n",
       "  )\n",
       "  (psi): ModuleList(\n",
       "    (0): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (8): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (9): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (10): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (11): PsiICA(\n",
       "      (m): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.25, inplace=False)\n",
       "        (2): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Dropout(p=0.25, inplace=False)\n",
       "        (5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward_h()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_i$ represent a single dimension (unmixed source, disentangled feature).\n",
    "\n",
    "The total model output:\n",
    "\n",
    "$$r(x, u) = \\sum\\limits_{i=1}^n \\psi_i(h_i(\\mathbf{x}), \\mathbf{u})$$\n",
    "\n",
    "The $\\mathbf{u}$ should be \"one-hot\" encoded.\n",
    "\n",
    "To **train** the model, use binary cross-entropy, check the quality (accuracy, loss) on the validation dataset.\n",
    "\n",
    "To **compare** the quality with InfoGAN, take correlations of values of $h_i$ with classes. Take the representation on the test dataset, look at the different values of $h_i$ within one class, try to find meaningful explanations.\n",
    "\n",
    "Compare the correlation between sources ($h_i$, features).\n",
    "\n",
    "So, we have the same weigths for $h_i$, and n outputs (Simple conv net).\n",
    "\n",
    "Then, for $\\phi_i$ we have $n$ different fully-connected networks, where we have the $\\mathbb{u}$ vector dimension + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.env_gmml)",
   "language": "python",
   "name": ".env_gmml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
